# Краулер
### Лабораторная работа #2
[[презентация]](https://www.dropbox.com/s/ay7b92bkjjng9yi/Task%202.pptx?dl=0) [[репозиторий]](https://github.com/Andrew414/crawlertask) [[english]](https://github.com/Andrew414/crawlertask/blob/master/README.md)

### Условие
Необходимо написать [краулер](https://en.wikipedia.org/wiki/Web_crawler), который может скачивать разделы Википедии с использованием многопоточности. Ваш краулер должен скачать один из [небольших разделов Википедии](https://en.wikipedia.org/wiki/Template:Wikipedias). Вам нужно выбрать один из [разделов](https://en.wikipedia.org/wiki/Wikipedia#Language_editions) и указать его в файле README.md в вашем персональном каталоге в репозитории. Желательно выбрать раздел, содержащий 100 и более страниц, чтобы можно было выполнить тесты производительности.

Краулер должен производить многопоточный [поиск в ширину](https://en.wikipedia.org/wiki/Breadth-first_search). Он должен начинать с главной страницы, парсить ее, находить все ссылки, удалять те из них, которые ведут на другие сайты, и добавлять оставшиеся ссылки в очередь обработки. Все страницы должны быть сохранены на диск. Можно применять некоторые оптимизации, относящиеся к Википедии, например, все параметры `HTTP` `GET` могут быть удалены, все ссылки, ведущие на `index.php` (e.g. [https://en.wikipedia.org/w/index.php?title=Star_Wars&action=edit](https://en.wikipedia.org/w/index.php?title=Star_Wars&action=edit)) тоже могут быть удалены, и т.д.

Картинки, стили CSS, шаблоны Википедии и другие ресурсы, не являющиеся страницами, не должны быть скачаны.

### Дополнительные задания
Четыре задачи:
* Добавить настройки к краулеру
 * Краулер должен использовать конфигурационный файл или параметры, чтобы настраивать свою работу и скачивать по-разному устроенные сайты:
  * Например, некоторые сайты (как Википедия) размещают разные страницы по разным URL-адресам. Другие (как [fpmi.bsu.by](http://fpmi.bsu.by/)) размещают все страницы по одному URL, сами страницы различаются только по параметрам `HTTP` `GET`. Например, http://fpmi.bsu.by/main.aspx?guid=18751 и http://fpmi.bsu.by/main.aspx?guid=21081 ведут на страницу `main.aspx`, но их содержимое разное из-за параметра `guid`
  * Некоторые сайты обрабатывают разные URL как один. Например http://fpmi.bsu.by/main.aspx?guid=18751 и http://fpmi.bsu.by/ru/main.aspx?guid=18751 имеют разные адреса, но ведут на одну и ту же страницу
  * Некоторые сайты различают ПРОПИСНЫЕ и строчные буквы в адресах, некоторые нет
  * В некоторых случаях картинки должны быть загружены, в некоторых нет
 * Краулер должен учитывать все эти варианты и предоставлять возможность гибкой настройки парсера (нужно ли преобразовывать URL, нужно ли удалять какие-то его части, нужно ли пропускать какие-то страницы и ссылки при парсинге, и т.д.).
* Добавить измерение производительности
 * Нужно сравнить скорость скачивания (скорость сети), парсинга (скорость процессора) и сохранения (скорость диска)
* Добавить графический интерфейс
 * Интерфейс должен показывать общую статистику, должен давать возможность управлять процессом (выбирать сайт для загрузки, начинать и приостанавливать скачивание), показывать статистику производительности и давать задавать настройки, если эти подзадачи реализованы
* Продемонстрировать скачанный архив сайта
 * Добавлять архив в репозиторий не нужно, нужно показать его на парах

### Репозиторий
[Репозиторий](https://github.com/Andrew414/crawlertask) содержит главную страницу [`README.md`](https://github.com/Andrew414/crawlertask/blob/master/README.rus.md) и четырнадцать персональных каталогов. Необходимо сделать копию (fork) репозитория и вносить все изменения только в свой персональный каталог. Все эти каталоги содержат пустой файл `.gitignore`, куда нужно вписать все временные файлы из лабораторной работы. Также, в этой лабораторной работе нужно создать файл `README.md` и записать туда выбранный раздел Википедии. Остальные файлы должны быть файлами кода (в т.ч. проекта).

### Сдача лабораторной работы
Можно выбрать любой язык программирования, IDE и фреймворки/библиотеки. Если для работы нужны какие-то нестандартные конфигурации или ПО, предоставьте инструкцию по настройке среды.

Запрещено использовать библиотеки и фреймворки, которые выполняют работу, близкую к условию лабораторной. Например, можно использовать средства для парсинга (регулярные выражения) или библиотеки, позволяющие скачать файл из Интернета, но нельзя использовать мощный настраиваемый парсер HTML или фреймворк, позволяющий скачать часть сайта вызовом одной функции.

Сдача через **Pull Request** из вашего клонированного репозитория. Pull Request должен содержать **только код, файлы проекта и информационные файлы (.gitignore, README.md)**. Результаты сборок, временные файлы и скачанный архив сайта должны быть исключены из коммитов.

### Оценка
Максимум за лабораторную работу - **10 баллов**.
- **3 балла** за написанный краулер (главная задача)
- **3 балла**, если работа сделана вовремя
- **1 балл** за подзадачу *настраиваемый краулер*
- **1 балл** за подзадачу *проверка производительности* 
- **1 балл** за подзадачу *графический интерфейс*
- **1 балл** за демонстрацию скачанного сайта

### Сроки
На выполнение лабораторной дается 4 недели (28 марта - 24 апреля). Лабораторная работа считается принятой, когда Pull Request переходит в состояние "approved". Дни, в течение которых лабораторная была на проверке, не учитываются.
![ ](https://i.snag.gy/lPOzf7.jpg)

Например, если вы сдаете лаборатрную через три недели после выдачи (17 апреля), но я даю комментарии только через неделю (24 апреля), у вас будет неделя на устранение недочетов, все дни после коммита до получения ревью не считаются


### Желаю удачи!